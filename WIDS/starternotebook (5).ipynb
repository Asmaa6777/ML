{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependencies\n",
    "\n",
    "### the notebook in a nutshell \n",
    "- importing \n",
    "software dependencies we need\n",
    "- load_data (please read its description it supposed to be very easy to deal with it)\n",
    "\n",
    "###  preprocesssing \n",
    "- I stored some important variables, then I used log transformation for long tailed distributions\n",
    "\n",
    "- iterative imputer just like what `MAAB` Advised\n",
    "\n",
    "- transormed the categorical features to be from the string type just to avoid some fitting isssues cause by the \n",
    "OneHotEncoder.\n",
    "\n",
    "- dropeed the `participant_ID` after storing it for stratification\n",
    "\n",
    "- I scaled quantative data and also FMRI data \n",
    "\n",
    "### modeling\n",
    "- base model is logisitc regression with l2 regulization (fancy name for punishing the model to stop overfitting to noise) \n",
    "- wrapped inside a multioutput classifier that just gives it the ability to predict 2 targets simontinously\n",
    "### validation \n",
    "- 5 splits stratifiedKFold works fine no need for grouping as paarticipants are unique \n",
    "- regular stratified  = 0.5901173137436236\n",
    "- RepeatedStratifiedKFold =0.5865925294336686 with 7 minutese delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np          \n",
    "from sklearn.metrics import f1_score  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import hmean\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import scipy\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer #it might not work directly if not try the following code line\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold,StratifiedKFold,RepeatedStratifiedKFold\n",
    "from pathlib import Path\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "just drop the path of your new data directory nothing more than that. \n",
    "\n",
    "\"\"\"\n",
    "path = '../data/new_data'\n",
    "\n",
    "def read_data(base_path:str) -> pd.DataFrame :\n",
    "    path = Path(base_path)\n",
    "    trc=pd.read_excel(path /'TRAIN_NEW'  / 'TRAIN_CATEGORICAL_METADATA_new.xlsx')\n",
    "    trq=pd.read_excel(path /'TRAIN_NEW'  / 'TRAIN_QUANTITATIVE_METADATA_new.xlsx')\n",
    "    trf=pd.read_csv(path   /'TRAIN_NEW'  / 'TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv')\n",
    "    trs=pd.read_excel(path /'TRAIN_NEW'  / 'TRAINING_SOLUTIONS.xlsx')  \n",
    "    tsc=pd.read_excel(path /'TEST'      / 'TEST_CATEGORICAL.xlsx')\n",
    "    tsq=pd.read_excel(path /'TEST'       / 'TEST_QUANTITATIVE_METADATA.xlsx')    \n",
    "    tsf=pd.read_csv(path   /'TEST'       / 'TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv')    \n",
    "    sub=pd.read_excel(path / 'SAMPLE_SUBMISSION.xlsx')    \n",
    "    dic=pd.read_excel(path /'Data Dictionary.xlsx')\n",
    "    return trc, trq, trf, trs, tsc, tsq, tsf, sub, dic\n",
    "\n",
    "trc, trq, trf, trs, tsc, tsq, tsf, sub, dic = read_data(base_path=path)\n",
    "\n",
    "# Data Merging \n",
    "cq = pd.merge(trc, trq, on='participant_id', how='left')\n",
    "feat = pd.merge(cq, trf, on='participant_id', how='left')  \n",
    "qc = pd.merge(tsc, tsq, on='participant_id', how='left')\n",
    "train = pd.merge(feat, trs, on='participant_id', how='left') \n",
    "test = pd.merge(qc, tsf, on='participant_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# highlighting important variables. \\n#note that I did\\'t deal with the quantative data trq as categorical\\n# I will use the OneHotEncoder for the categorical data as we have some data trap that I don\\'t think we can use the label encoder for. \\n\\nfor feature in trc.columns:\\n    train[feature] = train[feature].astype(object)\\ntrain_ids = train[\\'participant_id\\']\\ntest_ids = train[\\'participant_id\\'] # I will store them for later usage in grouping in validation why?  I don\\'t want the same user to appear in both train and test. \\nnum_feats = [feature for feature in train.columns if train[feature].dtype == \\'float64\\']\\ncat_feats = [feature for feature in train.columns if train[feature].dtype == \\'object\\'] # seperate categorical and numerical features help me reteriving them later easily for preprocessing.\\ntarget_cols = [\\'ADHD_Outcome\\', \\'Sex_F\\']\\ngroups = train_ids\\nlog_features = [f for f in num_feats if (train[f] >= 0).all() and scipy.stats.skew(train[f]) > 0] # I will apply log transformation for the skewed features (basic preprocessing our models assume normal bell curve probability distribution)\\n\\nencoder = OneHotEncoder(handle_unknown=\\'ignore\\',sparse_output=False)\\nle = LabelEncoder()\\nfor feature in log_features:\\n    train[feature] = np.log1p(train[feature])\\n    test[feature] = np.log1p(test[feature])\\n    \\n# Fill missing values with the mode for each column\\n\\nimputer = IterativeImputer(max_iter=10, random_state=7)\\nfor feature in train.columns:\\n    if feature not in target_cols and feature != \\'participant_id\\':\\n        train[feature] = imputer.fit_transform(train[[feature]])\\n        test[feature] = imputer.transform(test[[feature]])\\n\\n# Convert all categorical features to strings to avoid mixed types\\nfor feature in cat_feats:\\n    train[feature] = train[feature].astype(str)\\n    test[feature] = test[feature].astype(str)\\n\\nfor feature in cat_feats:\\n    if feature != \\'participant_id\\':\\n        \\n        train_encoded = encoder.fit_transform(train[[feature]])\\n        test_encoded = encoder.transform(test[[feature]])\\n        #print(f\"Encoding {feature} with OneHotEncoder\") #just to make sure I\\'m not encoding continuous features\\n        train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out([feature]))\\n        test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out([feature]))\\n        train = pd.concat([train.drop(columns=[feature]), train_encoded_df], axis=1)\\n        test = pd.concat([test.drop(columns=[feature]), test_encoded_df], axis=1)\\n\\n    else:\\n        train[feature] = le.fit_transform(train[feature])\\n        test[feature] = le.fit_transform(test[feature])        \\n  \\nfor df in (train,test):\\n    df.drop(columns=[\\'participant_id\\'], inplace=True)      \\n    \\n    #Scaling features\\nfrom sklearn.preprocessing import StandardScaler \\nscaler = StandardScaler()\\n\\nfor col in train.columns: \\n    if col not in target_cols and col not in cat_feats:\\n        train[col] = scaler.fit_transform(train[[col]])\\n        test[col] = scaler.transform(test[[col]])\\n'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#highlighting important variables. \n",
    "#note that I did't deal with the quantative data trq as categorical\n",
    "# I will use the OneHotEncoder for the categorical data as we have some data trap that I don't think we can use the label encoder for. \n",
    "\n",
    "for feature in trc.columns:\n",
    "    train[feature] = train[feature].astype(object)\n",
    "train_ids = train['participant_id']\n",
    "test_ids = train['participant_id'] # I will store them for later usage in grouping in validation why?  I don't want the same user to appear in both train and test. \n",
    "num_feats = [feature for feature in train.columns if train[feature].dtype == 'float64']\n",
    "cat_feats = [feature for feature in train.columns if train[feature].dtype == 'object'] # seperate categorical and numerical features help me reteriving them later easily for preprocessing.\n",
    "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
    "groups = train_ids\n",
    "log_features = [f for f in num_feats if (train[f] >= 0).all() and scipy.stats.skew(train[f]) > 0] # I will apply log transformation for the skewed features (basic preprocessing our models assume normal bell curve probability distribution)\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore',sparse_output=False)\n",
    "le = LabelEncoder()\n",
    "for feature in log_features:\n",
    "    train[feature] = np.log1p(train[feature])\n",
    "    test[feature] = np.log1p(test[feature])\n",
    "    \n",
    "# Fill missing values with the mode for each column\n",
    "for feature in train.columns:\n",
    "    if feature not in target_cols:\n",
    "        train[feature].fillna(train[feature].mode()[0], inplace=True)\n",
    "        test[feature].fillna(train[feature].mode()[0], inplace=True)  # Use train's mode to ensure consistency\n",
    "        \n",
    "\"\"\"imputer = IterativeImputer(max_iter=10, random_state=7)\n",
    "for feature in train.columns:\n",
    "    if feature not in target_cols and feature != 'participant_id':\n",
    "        train[feature] = imputer.fit_transform(train[[feature]])\n",
    "        test[feature] = imputer.transform(test[[feature]])\n",
    "\"\"\"\n",
    "\n",
    "#un comment the above code if you want to use the IterativeImputer. it delayes the process a bit maybe 2-3 minutes,\n",
    "# Convert all categorical features to strings to avoid mixed types\n",
    "for feature in cat_feats:\n",
    "    train[feature] = train[feature].astype(str)\n",
    "    test[feature] = test[feature].astype(str)\n",
    "\n",
    "for feature in cat_feats:\n",
    "    if feature != 'participant_id':\n",
    "        \n",
    "        train_encoded = encoder.fit_transform(train[[feature]])\n",
    "        test_encoded = encoder.transform(test[[feature]])\n",
    "        #print(f\"Encoding {feature} with OneHotEncoder\") #just to make sure I'm not encoding continuous features\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out([feature]))\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out([feature]))\n",
    "        train = pd.concat([train.drop(columns=[feature]), train_encoded_df], axis=1)\n",
    "        test = pd.concat([test.drop(columns=[feature]), test_encoded_df], axis=1)\n",
    "\n",
    "    else:\n",
    "        train[feature] = le.fit_transform(train[feature])\n",
    "        test[feature] = le.fit_transform(test[feature])        \n",
    "  \n",
    "for df in (train,test):\n",
    "    df.drop(columns=['participant_id'], inplace=True)      \n",
    "    \n",
    "    #Scaling features\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "for col in train.columns: \n",
    "    if col not in target_cols and col not in cat_feats:\n",
    "        train[col] = scaler.fit_transform(train[[col]])\n",
    "        test[col] = scaler.transform(test[[col]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores per target: {'ADHD_Outcome': 0.7090909090909091, 'Sex_F': 0.5}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6909090909090909, 'Sex_F': 0.5182926829268293}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.662379421221865, 'Sex_F': 0.5492537313432836}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6384364820846905, 'Sex_F': 0.49221183800623053}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7378048780487805, 'Sex_F': 0.49221183800623053}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6811145510835913, 'Sex_F': 0.48598130841121495}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6815286624203821, 'Sex_F': 0.5227963525835866}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7040498442367601, 'Sex_F': 0.5137614678899083}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6773162939297125, 'Sex_F': 0.5015479876160991}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7177914110429447, 'Sex_F': 0.5288753799392097}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.725, 'Sex_F': 0.5272727272727272}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6915887850467289, 'Sex_F': 0.4953560371517028}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7321428571428571, 'Sex_F': 0.5227963525835866}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6538461538461539, 'Sex_F': 0.524390243902439}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7306501547987616, 'Sex_F': 0.4827586206896552}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6666666666666666, 'Sex_F': 0.50920245398773}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7192429022082019, 'Sex_F': 0.5227963525835866}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.668769716088328, 'Sex_F': 0.5405405405405406}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6835443037974683, 'Sex_F': 0.5508982035928144}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7741935483870968, 'Sex_F': 0.4234527687296417}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.691358024691358, 'Sex_F': 0.5227963525835866}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7138461538461538, 'Sex_F': 0.5492537313432836}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7037037037037037, 'Sex_F': 0.4716981132075472}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6858974358974359, 'Sex_F': 0.4827586206896552}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6876971608832808, 'Sex_F': 0.524390243902439}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6751592356687898, 'Sex_F': 0.4716981132075472}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6867469879518072, 'Sex_F': 0.5317220543806647}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6978193146417445, 'Sex_F': 0.5046153846153846}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6731391585760518, 'Sex_F': 0.5198776758409785}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6975308641975309, 'Sex_F': 0.524390243902439}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7012195121951219, 'Sex_F': 0.4953560371517028}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6730769230769231, 'Sex_F': 0.5046153846153846}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6956521739130435, 'Sex_F': 0.5}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6514657980456026, 'Sex_F': 0.5198776758409785}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7305389221556886, 'Sex_F': 0.5333333333333333}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7088607594936709, 'Sex_F': 0.5789473684210527}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6773162939297125, 'Sex_F': 0.4716981132075472}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6559485530546624, 'Sex_F': 0.4906832298136646}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6990881458966566, 'Sex_F': 0.5061728395061729}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6871165644171779, 'Sex_F': 0.5015479876160991}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.662379421221865, 'Sex_F': 0.5272727272727272}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7100591715976331, 'Sex_F': 0.47648902821316613}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6903225806451613, 'Sex_F': 0.4906832298136646}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6557377049180327, 'Sex_F': 0.5333333333333333}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7204968944099379, 'Sex_F': 0.524390243902439}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6603174603174603, 'Sex_F': 0.5621301775147929}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.703030303030303, 'Sex_F': 0.4472843450479233}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7006369426751592, 'Sex_F': 0.48125}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7315634218289085, 'Sex_F': 0.5465465465465466}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7232704402515723, 'Sex_F': 0.5107692307692308}\n",
      "Mean F1-scores per target: [0.69394125 0.51019957]\n",
      "F1-score stds per target: [0.0266613  0.02857526]\n",
      "Standard deviations of test sets: [ADHD_Outcome    0.464573\n",
      "Sex_F           0.472377\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.477889\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.486040\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.469885\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.469885\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.467820\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.479168\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.476571\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.472867\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.480850\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.480409\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.470899\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.479168\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.479618\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.466736\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.475213\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.479168\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.483901\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.486433\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.444147\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.479168\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.486040\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.462886\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.466736\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.479618\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.462886\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.481611\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.473815\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.478347\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.479618\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.470899\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.473815\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.472377\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.478347\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.482043\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.492366\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.462886\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.469380\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.474297\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.472867\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.480409\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.464573\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.469380\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.482043\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.479618\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.488970\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.453797\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.466218\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.485392\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.475687\n",
      "dtype: float64]\n",
      "score mean  [0.69394125 0.51019957]\n",
      "overall harmonic mean  0.5865925294336686\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ayo so there is this modeling thing, the part are \n",
    "- base model is a RidgeClassifier with a regulizer (more on that on comments)\"\n",
    "- multioutput classifier is just a wrapper it sucks the base model and elevate its skills to make it output multiple targets.\n",
    "- there is a pipeline inside the multiooutput classifier that does the following:\n",
    "1. impute missing values with mean\n",
    "2. log transform the features that are skewed so bad (you know why now)\n",
    "3. scale the features to be between 0 and 1\n",
    "4. PCA to reduce the dimensionality of the features (970 components)\n",
    "   \n",
    "\"\"\"\n",
    "features = test.columns\n",
    "n_splits = 5 \n",
    "cv = StratifiedKFold(n_splits=n_splits)\n",
    "base_model = LogisticRegression(random_state=7,penalty='l2', C=0.02,class_weight=\"balanced\")\n",
    "\n",
    "model = MultiOutputClassifier(base_model)\n",
    "\n",
    "def validate(trainset, testset, target_cols):\n",
    "    weights = (trainset['Sex_F'] == 1) & (trainset['ADHD_Outcome'] == 1) * 2 + 1\n",
    "    model.fit(trainset.drop(columns=target_cols), trainset[target_cols],sample_weight=weights)\n",
    "    pred = model.predict(testset.drop(columns=target_cols))\n",
    "    valid_idx = testset[target_cols].notna().all(axis=1)\n",
    "    \n",
    "    valid_testset = testset.loc[valid_idx, target_cols]\n",
    "    valid_pred = pred[valid_idx]\n",
    "    f1_scores = [f1_score(valid_testset[col], valid_pred[:, i]) for i, col in enumerate(target_cols)]\n",
    "    \n",
    "    print(f\"F1-scores per target: {dict(zip(target_cols, f1_scores))}\")\n",
    "    \n",
    "    return f1_scores\n",
    "\n",
    "\n",
    "stds = []\n",
    "F1s = []\n",
    "\n",
    "for train_index, test_index in cv.split(train.drop(columns=target_cols), train[target_cols[0]]): \n",
    "    train_v, test_v = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "    stds.append(test_v[target_cols].std())\n",
    "    F1s.append(validate(train_v, test_v, target_cols))\n",
    "\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "print(\"Mean F1-scores per target:\", F1s.mean(axis=0))\n",
    "print(\"F1-score stds per target:\", F1s.std(axis=0))\n",
    "print(\"Standard deviations of test sets:\", stds)\n",
    "print(\"score mean \", np.mean(F1s, axis=0))\n",
    "score = hmean(F1s, axis=0)\n",
    "score = hmean(score, axis=0)\n",
    "print(\"overall harmonic mean \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(testdata, model):\n",
    "\n",
    "    model.fit(train.drop(columns=[\"ADHD_Outcome\", \"Sex_F\"], axis=1), train[[\"ADHD_Outcome\", \"Sex_F\"]])\n",
    "    y_pred = model.predict(test)\n",
    "    sub['ADHD_Outcome'] = y_pred[:, 0] \n",
    "    sub['Sex_F'] = y_pred[:, 1]        \n",
    "    sub.to_csv(f'../submissions/submission{score}.csv', index=False)\n",
    "    \n",
    "inference(test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
