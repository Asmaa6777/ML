{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca09355a",
   "metadata": {},
   "source": [
    "# LGBM and Downsampling using spreted models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e30d22",
   "metadata": {},
   "source": [
    " Score in privet leaderbord is : 0.75065 \n",
    "\n",
    "Score in public leaderbord is : 0.71057 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd4a2b",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aaa9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "from scipy.stats import hmean\n",
    "import scipy\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a1a55",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be44016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "just drop the path of your new data directory nothing more than that. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "path = r'C:\\Users\\Family\\Downloads\\widsdatathon2025 (3)'\n",
    "\n",
    "def read_data(base_path:str) -> pd.DataFrame :\n",
    "    path = Path(base_path)\n",
    "    trc=pd.read_excel(path /'TRAIN_NEW'  / 'TRAIN_CATEGORICAL_METADATA_new.xlsx')\n",
    "    trq=pd.read_excel(path /'TRAIN_NEW'  / 'TRAIN_QUANTITATIVE_METADATA_new.xlsx')\n",
    "    trf=pd.read_csv(path   /'TRAIN_NEW'  / 'TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv')\n",
    "    trs=pd.read_excel(path /'TRAIN_NEW'  / 'TRAINING_SOLUTIONS.xlsx')  \n",
    "    tsc=pd.read_excel(path /'TEST'      / 'TEST_CATEGORICAL.xlsx')\n",
    "    tsq=pd.read_excel(path /'TEST'       / 'TEST_QUANTITATIVE_METADATA.xlsx')    \n",
    "    tsf=pd.read_csv(path   /'TEST'       / 'TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv')    \n",
    "    sub=pd.read_excel(path / 'SAMPLE_SUBMISSION.xlsx')    \n",
    "    dic=pd.read_excel(path /'Data Dictionary.xlsx')\n",
    "    return trc, trq, trf, trs, tsc, tsq, tsf, sub, dic\n",
    "\n",
    "trc, trq, trf, trs, tsc, tsq, tsf, sub, dic = read_data(base_path=path)\n",
    "\n",
    "# Data Merging \n",
    "cq = pd.merge(trc, trq, on='participant_id', how='left')\n",
    "feat = pd.merge(cq, trf, on='participant_id', how='left')  \n",
    "qc = pd.merge(tsc, tsq, on='participant_id', how='left')\n",
    "train = pd.merge(feat, trs, on='participant_id', how='left') \n",
    "test = pd.merge(qc, tsf, on='participant_id', how='left')\n",
    "train_sex =train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b192f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train['participant_id']\n",
    "test_ids = test['participant_id'] # I will store them for later usage in grouping in validation why?  I don't want the same user to appear in both train and test. \n",
    "num_feats = trq # numerical features\n",
    "cat_feats = trc # seperate categorical and numerical features help me reteriving them later easily for preprocessing.\n",
    "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
    "groups = train_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cbc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.220e+00, tolerance: 4.923e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.402e+00, tolerance: 5.646e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+00, tolerance: 9.479e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.220e+00, tolerance: 4.923e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.402e+00, tolerance: 5.646e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+00, tolerance: 9.479e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.220e+00, tolerance: 4.923e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.402e+00, tolerance: 5.646e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+00, tolerance: 9.479e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.220e+00, tolerance: 4.923e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.402e+00, tolerance: 5.646e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+00, tolerance: 9.479e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.220e+00, tolerance: 4.923e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.402e+00, tolerance: 5.646e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+00, tolerance: 9.479e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+00, tolerance: 1.063e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.191e-01, tolerance: 1.249e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.220e+00, tolerance: 2.173e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 57.705368523078505, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63.64947708428372, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 70.45652162254555, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 78.25303920154693, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 87.18342166085495, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+00, tolerance: 1.063e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.191e-01, tolerance: 1.249e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.220e+00, tolerance: 2.173e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.452079902461264, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 61.975659622461535, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 68.29784536501393, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 75.53646806970937, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+00, tolerance: 1.063e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.191e-01, tolerance: 1.249e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.220e+00, tolerance: 2.173e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 59.38507271686103, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.38368596072542, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 72.25161778798793, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 80.1165836713626, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+00, tolerance: 1.063e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.191e-01, tolerance: 1.249e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.220e+00, tolerance: 2.173e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 58.458702997420914, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.34644913248485, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 71.08736002753722, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 78.80683661869261, tolerance: 54.62246194279837\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+00, tolerance: 1.063e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.191e-01, tolerance: 1.249e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Family\\Desktop\\WIDS-Com\\myvenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.220e+00, tolerance: 2.173e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train has 25 features/columns with missing values: ['PreInt_Demos_Fam_Child_Ethnicity', 'PreInt_Demos_Fam_Child_Race', 'MRI_Track_Scan_Location', 'Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ', 'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact', 'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan']\n",
    "Test  has 23 features/columns with missing values: ['PreInt_Demos_Fam_Child_Ethnicity', 'PreInt_Demos_Fam_Child_Race', 'Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ', 'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact', 'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial']\n",
    "fMRI has no missing values\n",
    "Extra columns in train: ['MRI_Track_Age_at_Scan', 'MRI_Track_Scan_Location']\n",
    "'''\n",
    "\n",
    "# Find columns with missing values only\n",
    "train_missing_features_to_impute = train.columns[train.isnull().any()].tolist() # List of features with missing values in train, only 25 and no missing data in fMRI data\n",
    "test_missing_features_to_impute = test.columns[test.isnull().any()].tolist() # List of features with missing values in test, only 23 and no missing data in fMRI data\n",
    "\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = IterativeImputer(estimator=LassoCV(random_state=42), max_iter=5, random_state=42)\n",
    "\n",
    "# Impute in-place for train\n",
    "if train_missing_features_to_impute:\n",
    "\ttrain[train_missing_features_to_impute] = imputer.fit_transform(train[train_missing_features_to_impute])\n",
    "\n",
    "# Impute in-place for test\n",
    "if test_missing_features_to_impute:\n",
    "\ttest[test_missing_features_to_impute] = imputer.fit_transform(test[test_missing_features_to_impute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd45b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sum(), test.isnull().sum().sum() # Check if there are any missing values left in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Only apply scaling to numerical columns that are not part of the target or categorical features\n",
    "numerical_features = [col for col in train.columns if col not in target_cols and col not in cat_feats]\n",
    "\n",
    "# Fit scaler on the numerical features of the train set and transform train and test sets\n",
    "train[numerical_features] = scaler.fit_transform(train[numerical_features])  # Fit and transform for train set\n",
    "test[numerical_features] = scaler.transform(test[numerical_features]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sum(), test.isnull().sum().sum() # Check if there are any missing values left in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eea20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sex = train['Sex_F']  \n",
    "y_adhd = train['ADHD_Outcome']  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568f463",
   "metadata": {},
   "source": [
    "### feature importance in sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59f066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>Basic_Demos_Enroll_Year</th>\n",
       "      <th>Basic_Demos_Study_Site</th>\n",
       "      <th>PreInt_Demos_Fam_Child_Ethnicity</th>\n",
       "      <th>PreInt_Demos_Fam_Child_Race</th>\n",
       "      <th>MRI_Track_Scan_Location</th>\n",
       "      <th>Barratt_Barratt_P1_Edu</th>\n",
       "      <th>Barratt_Barratt_P1_Occ</th>\n",
       "      <th>Barratt_Barratt_P2_Edu</th>\n",
       "      <th>Barratt_Barratt_P2_Occ</th>\n",
       "      <th>...</th>\n",
       "      <th>195throw_198thcolumn</th>\n",
       "      <th>195throw_199thcolumn</th>\n",
       "      <th>196throw_197thcolumn</th>\n",
       "      <th>196throw_198thcolumn</th>\n",
       "      <th>196throw_199thcolumn</th>\n",
       "      <th>197throw_198thcolumn</th>\n",
       "      <th>197throw_199thcolumn</th>\n",
       "      <th>198throw_199thcolumn</th>\n",
       "      <th>ADHD_Outcome</th>\n",
       "      <th>Sex_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00aIpNTbG5uh</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>17.903324</td>\n",
       "      <td>32.871543</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.185011</td>\n",
       "      <td>0.141726</td>\n",
       "      <td>-0.890926</td>\n",
       "      <td>-0.677929</td>\n",
       "      <td>-0.180457</td>\n",
       "      <td>-0.554190</td>\n",
       "      <td>-0.835889</td>\n",
       "      <td>-0.024380</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00fV0OyyoLfw</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.441008</td>\n",
       "      <td>-1.721379</td>\n",
       "      <td>-0.001464</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>-0.239836</td>\n",
       "      <td>-0.922257</td>\n",
       "      <td>0.310269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04X1eiS79T4B</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.387745</td>\n",
       "      <td>17.897575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172175</td>\n",
       "      <td>-0.930766</td>\n",
       "      <td>0.810557</td>\n",
       "      <td>-0.436211</td>\n",
       "      <td>0.606624</td>\n",
       "      <td>-1.385552</td>\n",
       "      <td>0.554995</td>\n",
       "      <td>0.364357</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05ocQutkURd6</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.791115</td>\n",
       "      <td>-1.136420</td>\n",
       "      <td>-0.252985</td>\n",
       "      <td>-0.394303</td>\n",
       "      <td>0.234110</td>\n",
       "      <td>0.063241</td>\n",
       "      <td>-0.648517</td>\n",
       "      <td>-0.567490</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06YUNBA9ZRLq</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.845904</td>\n",
       "      <td>22.432518</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505395</td>\n",
       "      <td>-0.061681</td>\n",
       "      <td>-0.278927</td>\n",
       "      <td>-0.204557</td>\n",
       "      <td>-0.283942</td>\n",
       "      <td>-0.328775</td>\n",
       "      <td>-1.684912</td>\n",
       "      <td>-0.969062</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>zwjJWCRzKhDz</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.136452</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.732856</td>\n",
       "      <td>1.016003</td>\n",
       "      <td>0.292482</td>\n",
       "      <td>1.063558</td>\n",
       "      <td>-0.303480</td>\n",
       "      <td>-0.081801</td>\n",
       "      <td>-1.617241</td>\n",
       "      <td>-1.109779</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>zwXD5v17Rx01</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421749</td>\n",
       "      <td>0.952747</td>\n",
       "      <td>0.605150</td>\n",
       "      <td>-1.623857</td>\n",
       "      <td>0.634374</td>\n",
       "      <td>-2.157031</td>\n",
       "      <td>-0.405763</td>\n",
       "      <td>-0.035614</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>zWzLCi3NTBTd</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400430</td>\n",
       "      <td>1.134626</td>\n",
       "      <td>-0.124110</td>\n",
       "      <td>-1.780372</td>\n",
       "      <td>0.531519</td>\n",
       "      <td>-2.287286</td>\n",
       "      <td>-1.525919</td>\n",
       "      <td>-2.118786</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>Zy9GTHDxUbXU</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>-0.359999</td>\n",
       "      <td>0.990114</td>\n",
       "      <td>-0.976870</td>\n",
       "      <td>-1.389992</td>\n",
       "      <td>-0.451771</td>\n",
       "      <td>-1.821987</td>\n",
       "      <td>0.961151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>Zye7yYRQohXi</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.698867</td>\n",
       "      <td>-1.194641</td>\n",
       "      <td>0.338708</td>\n",
       "      <td>-1.312060</td>\n",
       "      <td>-0.556121</td>\n",
       "      <td>0.335012</td>\n",
       "      <td>-0.705410</td>\n",
       "      <td>-1.421709</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213 rows × 19930 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     participant_id  Basic_Demos_Enroll_Year  Basic_Demos_Study_Site  \\\n",
       "0      00aIpNTbG5uh                     2019                       4   \n",
       "1      00fV0OyyoLfw                     2017                       1   \n",
       "2      04X1eiS79T4B                     2017                       1   \n",
       "3      05ocQutkURd6                     2018                       1   \n",
       "4      06YUNBA9ZRLq                     2018                       1   \n",
       "...             ...                      ...                     ...   \n",
       "1208   zwjJWCRzKhDz                     2019                       4   \n",
       "1209   zwXD5v17Rx01                     2018                       1   \n",
       "1210   zWzLCi3NTBTd                     2018                       3   \n",
       "1211   Zy9GTHDxUbXU                     2019                       4   \n",
       "1212   Zye7yYRQohXi                     2017                       1   \n",
       "\n",
       "      PreInt_Demos_Fam_Child_Ethnicity  PreInt_Demos_Fam_Child_Race  \\\n",
       "0                                  1.0                          0.0   \n",
       "1                                  0.0                          9.0   \n",
       "2                                  1.0                          2.0   \n",
       "3                                  3.0                          8.0   \n",
       "4                                  0.0                          1.0   \n",
       "...                                ...                          ...   \n",
       "1208                               1.0                          1.0   \n",
       "1209                               0.0                          0.0   \n",
       "1210                               2.0                          3.0   \n",
       "1211                               0.0                          1.0   \n",
       "1212                               0.0                          0.0   \n",
       "\n",
       "      MRI_Track_Scan_Location  Barratt_Barratt_P1_Edu  Barratt_Barratt_P1_Occ  \\\n",
       "0                         3.0                    21.0               45.000000   \n",
       "1                         2.0                    21.0                0.000000   \n",
       "2                         2.0                     9.0                0.000000   \n",
       "3                         2.0                    18.0               10.000000   \n",
       "4                         2.0                    12.0                0.000000   \n",
       "...                       ...                     ...                     ...   \n",
       "1208                      3.0                    12.0               15.136452   \n",
       "1209                      3.0                    21.0               40.000000   \n",
       "1210                      3.0                    21.0               40.000000   \n",
       "1211                      3.0                    18.0               35.000000   \n",
       "1212                      2.0                    18.0               35.000000   \n",
       "\n",
       "      Barratt_Barratt_P2_Edu  Barratt_Barratt_P2_Occ  ...  \\\n",
       "0                  17.903324               32.871543  ...   \n",
       "1                  21.000000               45.000000  ...   \n",
       "2                  12.387745               17.897575  ...   \n",
       "3                  18.000000                0.000000  ...   \n",
       "4                  13.845904               22.432518  ...   \n",
       "...                      ...                     ...  ...   \n",
       "1208               15.000000                5.000000  ...   \n",
       "1209               21.000000               40.000000  ...   \n",
       "1210               21.000000               35.000000  ...   \n",
       "1211               18.000000               45.000000  ...   \n",
       "1212               15.000000               35.000000  ...   \n",
       "\n",
       "      195throw_198thcolumn  195throw_199thcolumn  196throw_197thcolumn  \\\n",
       "0                -1.185011              0.141726             -0.890926   \n",
       "1                -1.441008             -1.721379             -0.001464   \n",
       "2                 0.172175             -0.930766              0.810557   \n",
       "3                -0.791115             -1.136420             -0.252985   \n",
       "4                -0.505395             -0.061681             -0.278927   \n",
       "...                    ...                   ...                   ...   \n",
       "1208              0.732856              1.016003              0.292482   \n",
       "1209              1.421749              0.952747              0.605150   \n",
       "1210              0.400430              1.134626             -0.124110   \n",
       "1211              0.007159             -0.359999              0.990114   \n",
       "1212             -0.698867             -1.194641              0.338708   \n",
       "\n",
       "      196throw_198thcolumn  196throw_199thcolumn  197throw_198thcolumn  \\\n",
       "0                -0.677929             -0.180457             -0.554190   \n",
       "1                 0.499990              0.305932             -0.239836   \n",
       "2                -0.436211              0.606624             -1.385552   \n",
       "3                -0.394303              0.234110              0.063241   \n",
       "4                -0.204557             -0.283942             -0.328775   \n",
       "...                    ...                   ...                   ...   \n",
       "1208              1.063558             -0.303480             -0.081801   \n",
       "1209             -1.623857              0.634374             -2.157031   \n",
       "1210             -1.780372              0.531519             -2.287286   \n",
       "1211             -0.976870             -1.389992             -0.451771   \n",
       "1212             -1.312060             -0.556121              0.335012   \n",
       "\n",
       "      197throw_199thcolumn  198throw_199thcolumn  ADHD_Outcome  Sex_F  \n",
       "0                -0.835889             -0.024380             1      0  \n",
       "1                -0.922257              0.310269             1      0  \n",
       "2                 0.554995              0.364357             0      1  \n",
       "3                -0.648517             -0.567490             0      1  \n",
       "4                -1.684912             -0.969062             1      0  \n",
       "...                    ...                   ...           ...    ...  \n",
       "1208             -1.617241             -1.109779             0      1  \n",
       "1209             -0.405763             -0.035614             1      0  \n",
       "1210             -1.525919             -2.118786             1      1  \n",
       "1211             -1.821987              0.961151             1      0  \n",
       "1212             -0.705410             -1.421709             0      1  \n",
       "\n",
       "[1213 rows x 19930 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 416, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.603047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5075431\n",
      "[LightGBM] [Info] Number of data points in the train set: 1213, number of used features: 19927\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "\n",
      "Number of important features for Sex: 2310\n",
      "\n",
      "Top 10 important features for Sex prediction:\n",
      "                    Feature  Importance\n",
      "19098  158throw_191thcolumn          21\n",
      "16356  114throw_199thcolumn          15\n",
      "13249   83throw_192thcolumn          14\n",
      "17753  133throw_171thcolumn          11\n",
      "18830  152throw_184thcolumn          10\n",
      "14901   99throw_124thcolumn          10\n",
      "19896  191throw_197thcolumn           9\n",
      "19321  164throw_189thcolumn           9\n",
      "7861     44throw_69thcolumn           8\n",
      "12304    76throw_80thcolumn           8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'train' and 'test' are your DataFrames\n",
    "# and target_cols is defined as ['ADHD_Outcome', 'Sex_F']\n",
    "\n",
    "# Prepare features (X) - drop targets and participant_id\n",
    "X = train.drop(columns=target_cols + ['participant_id'])\n",
    "\n",
    "# Convert categorical features to dummy variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Encode target (y_sex should be your target Series)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_sex)\n",
    "\n",
    "# Initialize and fit the LGBM model for Sex prediction\n",
    "model_sex = lgb.LGBMClassifier(\n",
    "    class_weight='balanced',  # Important for imbalanced data\n",
    "    random_state=42\n",
    ")\n",
    "model_sex.fit(X, y)\n",
    "\n",
    "# Get feature importances and create a DataFrame\n",
    "importance_df_sex = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model_sex.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Filter features with importance > 0 (or set a higher threshold)\n",
    "important_features_sex = importance_df_sex[importance_df_sex['Importance'] > 0]['Feature'].tolist()\n",
    "\n",
    "# Filter the data - ensure test has same features\n",
    "train_sex = train[important_features_sex]\n",
    "test_sex = test[important_features_sex]\n",
    "\n",
    "# Verify the filtered data\n",
    "print(f\"\\nNumber of important features for Sex: {len(important_features_sex)}\")\n",
    "print(\"\\nTop 10 important features for Sex prediction:\")\n",
    "print(importance_df_sex.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of > 0 threshold, consider:\n",
    "threshold = importance_df_sex['Importance'].quantile(0.98)  \n",
    "important_features_sex = importance_df_sex[importance_df_sex['Importance'] > threshold]['Feature'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c2068",
   "metadata": {},
   "source": [
    "# Modeling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33484580",
   "metadata": {},
   "source": [
    "**First, I trained a separate LightGBM model for each target, using bagging techniques to handle the class imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ef379",
   "metadata": {},
   "source": [
    "### SEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687818f",
   "metadata": {},
   "source": [
    "This code trains a LightGBM model to predict y_sex using a 5-bag, 5-fold ensemble with class balancing. In each fold, the majority class is downsampled to match the minority, helping with imbalance. Predictions are averaged across all bags and folds, and performance is measured using the macro F1-score for a fair evaluation of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e6beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Bag 1\n",
      "#########################\n",
      "=> Fold 1, => Fold 2, => Fold 3, => Fold 4, => Fold 5, \n",
      "Bag 1 Avg F1 - Sex: 0.7486\n",
      "#########################\n",
      "### Bag 2\n",
      "#########################\n",
      "=> Fold 1, => Fold 2, => Fold 3, => Fold 4, => Fold 5, \n",
      "Bag 2 Avg F1 - Sex: 0.7508\n",
      "#########################\n",
      "### Bag 3\n",
      "#########################\n",
      "=> Fold 1, => Fold 2, => Fold 3, => Fold 4, => Fold 5, \n",
      "Bag 3 Avg F1 - Sex: 0.7543\n",
      "#########################\n",
      "### Bag 4\n",
      "#########################\n",
      "=> Fold 1, => Fold 2, => Fold 3, => Fold 4, => Fold 5, \n",
      "Bag 4 Avg F1 - Sex: 0.7350\n",
      "#########################\n",
      "### Bag 5\n",
      "#########################\n",
      "=> Fold 1, => Fold 2, => Fold 3, => Fold 4, => Fold 5, \n",
      "Bag 5 Avg F1 - Sex: 0.7443\n",
      "\n",
      "##################################################\n",
      "Final Ensemble F1 - Sex: 0.7909\n",
      "Mean Bag F1 - Sex: 0.7466 ± 0.0066\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Initialize storage\n",
    "oof_sex = np.zeros(len(important_features_sex))\n",
    "models_sep_sex = {}  # Initialize models_sep_sex as an empty dictionary\n",
    "bag_f1_sex = []\n",
    "\n",
    "BAGS = 5\n",
    "FOLDS = 5\n",
    "\n",
    "# Prepare data\n",
    "important_features_sex_filtered = important_features_sex.reset_index(drop=True)\n",
    "y_sex = y_sex.reset_index(drop=True)\n",
    "\n",
    "for bag in range(BAGS):\n",
    "    print('#'*25)\n",
    "    print(f'### Bag {bag+1}')\n",
    "    print('#'*25)\n",
    "    \n",
    "    models_sep_sex[bag] = []\n",
    "    bag_pred_sex = np.zeros(len(important_features_sex))\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=bag*FOLDS)\n",
    "    fold_f1_sex = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X=important_features_sex_filtered, y=y_sex)):\n",
    "        print(f'=> Fold {fold+1}, ', end='')\n",
    "\n",
    "        # Dynamic class balancing\n",
    "        y_important_features_sex = y_sex.iloc[train_idx]\n",
    "        class_counts = y_important_features_sex.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = 1 - majority_class\n",
    "        \n",
    "        majority_samples = y_important_features_sex[y_important_features_sex == majority_class]\n",
    "        minority_samples = y_important_features_sex[y_important_features_sex == minority_class]\n",
    "        \n",
    "        n_samples = len(minority_samples)\n",
    "        if len(majority_samples) < n_samples:\n",
    "            n_samples = len(majority_samples)\n",
    "        \n",
    "        downsampled_majority = majority_samples.sample(\n",
    "            n=n_samples, \n",
    "            replace=len(majority_samples) < len(minority_samples),\n",
    "            random_state=bag*BAGS+fold\n",
    "        )\n",
    "        \n",
    "        sex_train_idx = minority_samples.index.union(downsampled_majority.index)\n",
    "        X_important_features_sex = important_features_sex_filtered.iloc[sex_train_idx]\n",
    "        y_important_features_sex_balanced = y_sex.iloc[sex_train_idx]\n",
    "        X_valid_sex = important_features_sex_filtered.iloc[valid_idx]\n",
    "        y_valid_sex = y_sex.iloc[valid_idx]\n",
    "\n",
    "        # Model parameters\n",
    "        model_params = {\n",
    "            'n_estimators': 300,\n",
    "            'learning_rate': 0.175, \n",
    "            'metric': 'cross_entropy_lambda',\n",
    "            'objective': None,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'reg_alpha': 0.0,\n",
    "            'reg_lambda': 0.0009,\n",
    "            'early_stopping_round': 50,\n",
    "            'feature_fraction': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'verbose': -1\n",
    "        }\n",
    "\n",
    "        model_sep_sex = LGBMClassifier(**model_params)\n",
    "        model_sep_sex.fit(X_important_features_sex, y_important_features_sex_balanced,\n",
    "                          eval_set=[(X_valid_sex, y_valid_sex)])\n",
    "\n",
    "        sex_pred = model_sep_sex.predict(X_valid_sex)\n",
    "        fold_f1_sex.append(f1_score(y_valid_sex, sex_pred, average='macro'))\n",
    "        \n",
    "        bag_pred_sex[valid_idx] = model_sep_sex.predict_proba(X_valid_sex)[:, 1]\n",
    "        models_sep_sex[bag].append(model_sep_sex)\n",
    "\n",
    "    bag_avg_f1_sex = np.mean(fold_f1_sex)\n",
    "    bag_f1_sex.append(bag_avg_f1_sex)\n",
    "    print(f\"\\nBag {bag+1} Avg F1 - Sex: {bag_avg_f1_sex:.4f}\")\n",
    "    oof_sex += bag_pred_sex / BAGS\n",
    "\n",
    "# Final evaluation\n",
    "final_f1_sex = f1_score(y_sex, (oof_sex > 0.5).astype(int), average='macro')\n",
    "print('\\n' + '#'*50)\n",
    "print(f\"Final Ensemble F1 - Sex: {final_f1_sex:.4f}\")\n",
    "print(f\"Mean Bag F1 - Sex: {np.mean(bag_f1_sex):.4f} ± {np.std(bag_f1_sex):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a527a",
   "metadata": {},
   "source": [
    "### ADHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84280ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only the columns from `trf` that are present in `train`\n",
    "columns_to_drop = [col for col in trf.columns if col in train.columns]\n",
    "train_adhd = train.drop(columns=columns_to_drop)\n",
    "\n",
    "# Drop the target columns\n",
    "train_adhd = train_adhd.drop(columns=target_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e33da",
   "metadata": {},
   "source": [
    "### LGBM WITH DOWNSAMPLING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4c0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Bag 1\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8013\n",
      "=> Fold 2, F1: 0.8013\n",
      "=> Fold 2, F1: 0.7682\n",
      "=> Fold 3, F1: 0.7682\n",
      "=> Fold 3, F1: 0.7810\n",
      "=> Fold 4, F1: 0.7810\n",
      "=> Fold 4, F1: 0.8037\n",
      "=> Fold 5, F1: 0.8037\n",
      "=> Fold 5, F1: 0.8287\n",
      "\n",
      "Bag 1 Avg F1: 0.7966\n",
      "#########################\n",
      "### Bag 2\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8287\n",
      "\n",
      "Bag 1 Avg F1: 0.7966\n",
      "#########################\n",
      "### Bag 2\n",
      "#########################\n",
      "=> Fold 1, F1: 0.7834\n",
      "=> Fold 2, F1: 0.7834\n",
      "=> Fold 2, F1: 0.8113\n",
      "=> Fold 3, F1: 0.8113\n",
      "=> Fold 3, F1: 0.7987\n",
      "=> Fold 4, F1: 0.7987\n",
      "=> Fold 4, F1: 0.7582\n",
      "=> Fold 5, F1: 0.7582\n",
      "=> Fold 5, F1: 0.8202\n",
      "\n",
      "Bag 2 Avg F1: 0.7944\n",
      "#########################\n",
      "### Bag 3\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8202\n",
      "\n",
      "Bag 2 Avg F1: 0.7944\n",
      "#########################\n",
      "### Bag 3\n",
      "#########################\n",
      "=> Fold 1, F1: 0.7974\n",
      "=> Fold 2, F1: 0.7974\n",
      "=> Fold 2, F1: 0.8091\n",
      "=> Fold 3, F1: 0.8091\n",
      "=> Fold 3, F1: 0.7925\n",
      "=> Fold 4, F1: 0.7925\n",
      "=> Fold 4, F1: 0.7821\n",
      "=> Fold 5, F1: 0.7821\n",
      "=> Fold 5, F1: 0.7923\n",
      "\n",
      "Bag 3 Avg F1: 0.7947\n",
      "#########################\n",
      "### Bag 4\n",
      "#########################\n",
      "=> Fold 1, F1: 0.7923\n",
      "\n",
      "Bag 3 Avg F1: 0.7947\n",
      "#########################\n",
      "### Bag 4\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8125\n",
      "=> Fold 2, F1: 0.8125\n",
      "=> Fold 2, F1: 0.7848\n",
      "=> Fold 3, F1: 0.7848\n",
      "=> Fold 3, F1: 0.7826\n",
      "=> Fold 4, F1: 0.7826\n",
      "=> Fold 4, F1: 0.8000\n",
      "=> Fold 5, F1: 0.8000\n",
      "=> Fold 5, F1: 0.7829\n",
      "\n",
      "Bag 4 Avg F1: 0.7926\n",
      "#########################\n",
      "### Bag 5\n",
      "#########################\n",
      "=> Fold 1, F1: 0.7829\n",
      "\n",
      "Bag 4 Avg F1: 0.7926\n",
      "#########################\n",
      "### Bag 5\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8115\n",
      "=> Fold 2, F1: 0.8115\n",
      "=> Fold 2, F1: 0.8103\n",
      "=> Fold 3, F1: 0.8103\n",
      "=> Fold 3, F1: 0.7821\n",
      "=> Fold 4, F1: 0.7821\n",
      "=> Fold 4, F1: 0.7821\n",
      "=> Fold 5, F1: 0.7821\n",
      "=> Fold 5, F1: 0.8131\n",
      "\n",
      "Bag 5 Avg F1: 0.7998\n",
      "\n",
      "##################################################\n",
      "==== Overall Results ===\n",
      "Mean F1: 0.7956\n",
      "F1 std: 0.0166\n",
      "\n",
      "Standard deviations of test sets:\n",
      "Fold 1: 0.4646\n",
      "Fold 2: 0.4662\n",
      "Fold 3: 0.4662\n",
      "Fold 4: 0.4651\n",
      "Fold 5: 0.4651\n",
      "Fold 6: 0.4646\n",
      "Fold 7: 0.4662\n",
      "Fold 8: 0.4662\n",
      "Fold 9: 0.4651\n",
      "Fold 10: 0.4651\n",
      "Fold 11: 0.4646\n",
      "Fold 12: 0.4662\n",
      "Fold 13: 0.4662\n",
      "Fold 14: 0.4651\n",
      "Fold 15: 0.4651\n",
      "Fold 16: 0.4646\n",
      "Fold 17: 0.4662\n",
      "Fold 18: 0.4662\n",
      "Fold 19: 0.4651\n",
      "Fold 20: 0.4651\n",
      "Fold 21: 0.4646\n",
      "Fold 22: 0.4662\n",
      "Fold 23: 0.4662\n",
      "Fold 24: 0.4651\n",
      "Fold 25: 0.4651\n",
      "\n",
      "Final OOF F1: 0.8084\n",
      "F1: 0.8131\n",
      "\n",
      "Bag 5 Avg F1: 0.7998\n",
      "\n",
      "##################################################\n",
      "==== Overall Results ===\n",
      "Mean F1: 0.7956\n",
      "F1 std: 0.0166\n",
      "\n",
      "Standard deviations of test sets:\n",
      "Fold 1: 0.4646\n",
      "Fold 2: 0.4662\n",
      "Fold 3: 0.4662\n",
      "Fold 4: 0.4651\n",
      "Fold 5: 0.4651\n",
      "Fold 6: 0.4646\n",
      "Fold 7: 0.4662\n",
      "Fold 8: 0.4662\n",
      "Fold 9: 0.4651\n",
      "Fold 10: 0.4651\n",
      "Fold 11: 0.4646\n",
      "Fold 12: 0.4662\n",
      "Fold 13: 0.4662\n",
      "Fold 14: 0.4651\n",
      "Fold 15: 0.4651\n",
      "Fold 16: 0.4646\n",
      "Fold 17: 0.4662\n",
      "Fold 18: 0.4662\n",
      "Fold 19: 0.4651\n",
      "Fold 20: 0.4651\n",
      "Fold 21: 0.4646\n",
      "Fold 22: 0.4662\n",
      "Fold 23: 0.4662\n",
      "Fold 24: 0.4651\n",
      "Fold 25: 0.4651\n",
      "\n",
      "Final OOF F1: 0.8084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import hmean\n",
    "\n",
    "models = {}\n",
    "bag_f1_scores = []\n",
    "stds = []\n",
    "F1s = []\n",
    "\n",
    "BAGS = 5\n",
    "FOLDS = 5\n",
    "target_col = 'ADHD_Outcome'\n",
    "\n",
    "# Prepare data\n",
    "X = train.drop(columns=[target_col, 'participant_id', 'Sex_F'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = train[target_col]\n",
    "\n",
    "for bag in range(BAGS):\n",
    "    print('#'*25)\n",
    "    print(f'### Bag {bag+1}')\n",
    "    print('#'*25)\n",
    "    \n",
    "    models[bag] = []\n",
    "    bag_pred = np.zeros(len(X))\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=bag*FOLDS)\n",
    "    fold_f1s = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f'=> Fold {fold}, ', end='')\n",
    "\n",
    "        # Your exact downsampling technique\n",
    "        y_train = y.iloc[train_idx]\n",
    "        class_counts = y_train.value_counts()\n",
    "        \n",
    "        if len(class_counts) > 1:\n",
    "            majority_class = class_counts.idxmax()\n",
    "            minority_class = 1 - majority_class\n",
    "            majority_samples = y_train[y_train == majority_class]\n",
    "            minority_samples = y_train[y_train == minority_class]\n",
    "            \n",
    "            n_samples = len(minority_samples)\n",
    "            downsampled_majority = majority_samples.sample(\n",
    "                n=n_samples, \n",
    "                replace=len(majority_samples) < len(minority_samples),\n",
    "                random_state=bag*BAGS+fold\n",
    "            )\n",
    "            balanced_idx = minority_samples.index.union(downsampled_majority.index)\n",
    "            X_train = X.iloc[balanced_idx]\n",
    "            y_train_balanced = y.iloc[balanced_idx]\n",
    "        else:\n",
    "            X_train = X.iloc[train_idx]\n",
    "            y_train_balanced = y.iloc[train_idx]\n",
    "\n",
    "        # Model training (unchanged)\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.175,\n",
    "            num_leaves=31,\n",
    "            reg_alpha=0.0,\n",
    "            reg_lambda=0.0009,\n",
    "            feature_fraction=0.1,\n",
    "            subsample=0.8,\n",
    "            verbose=-1,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        model.fit(X_train, y_train_balanced)\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = model.predict(X.iloc[valid_idx])\n",
    "        fold_f1 = f1_score(y.iloc[valid_idx], pred)\n",
    "        fold_f1s.append(fold_f1)\n",
    "        print(f\"F1: {fold_f1:.4f}\")\n",
    "        \n",
    "        # Store predictions\n",
    "        bag_pred[valid_idx] = model.predict_proba(X.iloc[valid_idx])[:, 1]\n",
    "        models[bag].append(model)\n",
    "        \n",
    "        # Store validation set std for this fold\n",
    "        stds.append(y.iloc[valid_idx].std())\n",
    "    \n",
    "    # Bag-level results\n",
    "    bag_avg_f1 = np.mean(fold_f1s)\n",
    "    bag_f1_scores.append(bag_avg_f1)\n",
    "    print(f\"\\nBag {bag+1} Avg F1: {bag_avg_f1:.4f}\")\n",
    "    oof_predictions += bag_pred / BAGS\n",
    "    F1s.append(fold_f1s)\n",
    "\n",
    "# Convert to numpy array for calculations\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "# Final evaluation (aligned with your original validation structure)\n",
    "print('\\n' + '#'*50)\n",
    "print(\"==== Overall Results ===\")\n",
    "print(f\"Mean F1: {np.mean(F1s):.4f}\")\n",
    "print(f\"F1 std: {np.std(F1s):.4f}\")\n",
    "\n",
    "print(\"\\nStandard deviations of test sets:\")\n",
    "for i, std in enumerate(stds):\n",
    "    print(f\"Fold {i+1}: {std:.4f}\")\n",
    "\n",
    "# Final OOF score calculation\n",
    "final_preds = (oof_predictions > 0.5).astype(int)\n",
    "final_f1 = f1_score(y, final_preds)\n",
    "print(f\"\\nFinal OOF F1: {final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7e905",
   "metadata": {},
   "source": [
    "### LGBM WITHOUT DOWNSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Bag 1\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8453\n",
      "=> Fold 2, F1: 0.8385\n",
      "=> Fold 3, F1: 0.8689\n",
      "=> Fold 4, F1: 0.8556\n",
      "=> Fold 5, F1: 0.8501\n",
      "\n",
      "Bag 1 Avg F1: 0.8517\n",
      "#########################\n",
      "### Bag 2\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8478\n",
      "=> Fold 2, F1: 0.8315\n",
      "=> Fold 3, F1: 0.8500\n",
      "=> Fold 4, F1: 0.8453\n",
      "=> Fold 5, F1: 0.8595\n",
      "\n",
      "Bag 2 Avg F1: 0.8468\n",
      "#########################\n",
      "### Bag 3\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8485\n",
      "=> Fold 2, F1: 0.8476\n",
      "=> Fold 3, F1: 0.8289\n",
      "=> Fold 4, F1: 0.8338\n",
      "=> Fold 5, F1: 0.8595\n",
      "\n",
      "Bag 3 Avg F1: 0.8437\n",
      "#########################\n",
      "### Bag 4\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8329\n",
      "=> Fold 2, F1: 0.8541\n",
      "=> Fold 3, F1: 0.8466\n",
      "=> Fold 4, F1: 0.8232\n",
      "=> Fold 5, F1: 0.8556\n",
      "\n",
      "Bag 4 Avg F1: 0.8425\n",
      "#########################\n",
      "### Bag 5\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8418\n",
      "=> Fold 2, F1: 0.8871\n",
      "=> Fold 3, F1: 0.8509\n",
      "=> Fold 4, F1: 0.8556\n",
      "=> Fold 5, F1: 0.8315\n",
      "\n",
      "Bag 5 Avg F1: 0.8534\n",
      "\n",
      "##################################################\n",
      "==== Overall Results ===\n",
      "Mean F1 across bags: 0.8476 ± 0.0043\n",
      "\n",
      "Fold-wise standard deviations:\n",
      "Fold 1: 0.4646\n",
      "Fold 2: 0.4662\n",
      "Fold 3: 0.4662\n",
      "Fold 4: 0.4651\n",
      "Fold 5: 0.4651\n",
      "\n",
      "Final OOF F1: 0.8506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Initialize storage\n",
    "oof_predictions = np.zeros(len(train))\n",
    "models = {}\n",
    "bag_f1_scores = []\n",
    "stds = []\n",
    "F1s = []\n",
    "\n",
    "BAGS = 5\n",
    "FOLDS = 5\n",
    "target_col = 'ADHD_Outcome'\n",
    "\n",
    "# Prepare data\n",
    "X = train.drop(columns=[target_col, 'participant_id', 'Sex_F'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = train[target_col]\n",
    "\n",
    "for bag in range(BAGS):\n",
    "    print('#'*25)\n",
    "    print(f'### Bag {bag+1}')\n",
    "    print('#'*25)\n",
    "    \n",
    "    models[bag] = []\n",
    "    bag_pred = np.zeros(len(X))\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=bag*FOLDS)\n",
    "    fold_f1s = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f'=> Fold {fold}, ', end='')\n",
    "\n",
    "        # Model training WITHOUT downsampling\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.175,\n",
    "            num_leaves=31,\n",
    "            reg_alpha=0.0,\n",
    "            reg_lambda=0.0009,\n",
    "            feature_fraction=0.1,\n",
    "            subsample=0.8,\n",
    "            verbose=-1,\n",
    "            class_weight='balanced'  # Handling imbalance through class weights\n",
    "        )\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = model.predict(X.iloc[valid_idx])\n",
    "        fold_f1 = f1_score(y.iloc[valid_idx], pred)\n",
    "        fold_f1s.append(fold_f1)\n",
    "        print(f\"F1: {fold_f1:.4f}\")\n",
    "        \n",
    "        # Store predictions\n",
    "        bag_pred[valid_idx] = model.predict_proba(X.iloc[valid_idx])[:, 1]\n",
    "        models[bag].append(model)\n",
    "        \n",
    "        # Store validation set std for this fold\n",
    "        stds.append(y.iloc[valid_idx].std())\n",
    "    \n",
    "    # Bag-level results\n",
    "    bag_avg_f1 = np.mean(fold_f1s)\n",
    "    bag_f1_scores.append(bag_avg_f1)\n",
    "    print(f\"\\nBag {bag+1} Avg F1: {bag_avg_f1:.4f}\")\n",
    "    oof_predictions += bag_pred / BAGS\n",
    "    F1s.append(fold_f1s)\n",
    "\n",
    "# Final evaluation\n",
    "print('\\n' + '#'*50)\n",
    "print(\"==== Overall Results ===\")\n",
    "print(f\"Mean F1 across bags: {np.mean(bag_f1_scores):.4f} ± {np.std(bag_f1_scores):.4f}\")\n",
    "\n",
    "print(\"\\nFold-wise standard deviations:\")\n",
    "for i, std in enumerate(stds[:FOLDS]):  # Show first fold's std as representative\n",
    "    print(f\"Fold {i+1}: {std:.4f}\")\n",
    "\n",
    "# Final OOF score\n",
    "final_preds = (oof_predictions > 0.5).astype(int)\n",
    "final_f1 = f1_score(y, final_preds)\n",
    "print(f\"\\nFinal OOF F1: {final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e61c8",
   "metadata": {},
   "source": [
    "## ONLY TRAIN_ADHD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837856c",
   "metadata": {},
   "source": [
    "here i traind the data with only TRAIN_ADHD and those the features without Fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Basic_Demos_Enroll_Year</th>\n",
       "      <th>Basic_Demos_Study_Site</th>\n",
       "      <th>PreInt_Demos_Fam_Child_Ethnicity</th>\n",
       "      <th>PreInt_Demos_Fam_Child_Race</th>\n",
       "      <th>MRI_Track_Scan_Location</th>\n",
       "      <th>Barratt_Barratt_P1_Edu</th>\n",
       "      <th>Barratt_Barratt_P1_Occ</th>\n",
       "      <th>Barratt_Barratt_P2_Edu</th>\n",
       "      <th>Barratt_Barratt_P2_Occ</th>\n",
       "      <th>EHQ_EHQ_Total</th>\n",
       "      <th>ColorVision_CV_Score</th>\n",
       "      <th>APQ_P_APQ_P_CP</th>\n",
       "      <th>APQ_P_APQ_P_ID</th>\n",
       "      <th>APQ_P_APQ_P_INV</th>\n",
       "      <th>APQ_P_APQ_P_OPD</th>\n",
       "      <th>APQ_P_APQ_P_PM</th>\n",
       "      <th>APQ_P_APQ_P_PP</th>\n",
       "      <th>SDQ_SDQ_Conduct_Problems</th>\n",
       "      <th>SDQ_SDQ_Difficulties_Total</th>\n",
       "      <th>SDQ_SDQ_Emotional_Problems</th>\n",
       "      <th>SDQ_SDQ_Externalizing</th>\n",
       "      <th>SDQ_SDQ_Generating_Impact</th>\n",
       "      <th>SDQ_SDQ_Hyperactivity</th>\n",
       "      <th>SDQ_SDQ_Internalizing</th>\n",
       "      <th>SDQ_SDQ_Peer_Problems</th>\n",
       "      <th>SDQ_SDQ_Prosocial</th>\n",
       "      <th>MRI_Track_Age_at_Scan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>17.903324</td>\n",
       "      <td>32.871543</td>\n",
       "      <td>0.818599</td>\n",
       "      <td>-0.200460</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>0.466715</td>\n",
       "      <td>0.870819</td>\n",
       "      <td>-1.195718</td>\n",
       "      <td>0.676164</td>\n",
       "      <td>0.492507</td>\n",
       "      <td>0.459262</td>\n",
       "      <td>0.737479</td>\n",
       "      <td>0.776497</td>\n",
       "      <td>0.823227</td>\n",
       "      <td>0.320435</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.400556</td>\n",
       "      <td>-0.132149</td>\n",
       "      <td>0.603281</td>\n",
       "      <td>1.052928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.662301</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>-0.372253</td>\n",
       "      <td>-0.985375</td>\n",
       "      <td>2.118640</td>\n",
       "      <td>2.233637</td>\n",
       "      <td>1.479851</td>\n",
       "      <td>1.451997</td>\n",
       "      <td>1.199700</td>\n",
       "      <td>0.776497</td>\n",
       "      <td>1.309451</td>\n",
       "      <td>0.320435</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.686599</td>\n",
       "      <td>0.348525</td>\n",
       "      <td>0.123977</td>\n",
       "      <td>1.088973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.387745</td>\n",
       "      <td>17.897575</td>\n",
       "      <td>0.549071</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>2.144650</td>\n",
       "      <td>-0.572888</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>1.844269</td>\n",
       "      <td>0.821621</td>\n",
       "      <td>0.459262</td>\n",
       "      <td>1.815994</td>\n",
       "      <td>2.167026</td>\n",
       "      <td>0.580116</td>\n",
       "      <td>2.108335</td>\n",
       "      <td>0.522620</td>\n",
       "      <td>2.688906</td>\n",
       "      <td>2.271221</td>\n",
       "      <td>-0.355327</td>\n",
       "      <td>0.773597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683936</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>-0.651909</td>\n",
       "      <td>0.458331</td>\n",
       "      <td>-0.894413</td>\n",
       "      <td>0.676164</td>\n",
       "      <td>0.821621</td>\n",
       "      <td>-1.029841</td>\n",
       "      <td>-1.111402</td>\n",
       "      <td>-1.077543</td>\n",
       "      <td>-1.121665</td>\n",
       "      <td>-1.467465</td>\n",
       "      <td>-0.908267</td>\n",
       "      <td>-0.743619</td>\n",
       "      <td>-0.132149</td>\n",
       "      <td>-0.834631</td>\n",
       "      <td>-0.566931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.845904</td>\n",
       "      <td>22.432518</td>\n",
       "      <td>-1.203370</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>3.157189</td>\n",
       "      <td>-0.372253</td>\n",
       "      <td>-0.985375</td>\n",
       "      <td>1.214724</td>\n",
       "      <td>-0.881308</td>\n",
       "      <td>-0.494837</td>\n",
       "      <td>1.948365</td>\n",
       "      <td>1.661920</td>\n",
       "      <td>2.167026</td>\n",
       "      <td>1.795674</td>\n",
       "      <td>1.393175</td>\n",
       "      <td>1.238063</td>\n",
       "      <td>0.972643</td>\n",
       "      <td>-0.612823</td>\n",
       "      <td>-1.793238</td>\n",
       "      <td>-1.572278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.136452</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.818599</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>0.187059</td>\n",
       "      <td>-0.779131</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>-0.686624</td>\n",
       "      <td>-1.153066</td>\n",
       "      <td>1.451997</td>\n",
       "      <td>1.199700</td>\n",
       "      <td>1.240007</td>\n",
       "      <td>1.309451</td>\n",
       "      <td>0.320435</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.686599</td>\n",
       "      <td>-0.132149</td>\n",
       "      <td>0.123977</td>\n",
       "      <td>0.018928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-0.259717</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>1.305682</td>\n",
       "      <td>0.458331</td>\n",
       "      <td>0.913419</td>\n",
       "      <td>-0.297256</td>\n",
       "      <td>-0.165722</td>\n",
       "      <td>-0.037106</td>\n",
       "      <td>-1.111402</td>\n",
       "      <td>-1.077543</td>\n",
       "      <td>-0.635442</td>\n",
       "      <td>-1.467465</td>\n",
       "      <td>-0.908267</td>\n",
       "      <td>-1.315707</td>\n",
       "      <td>-1.093497</td>\n",
       "      <td>-0.355327</td>\n",
       "      <td>-1.327849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-2.012158</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>1.585338</td>\n",
       "      <td>-1.191619</td>\n",
       "      <td>0.612114</td>\n",
       "      <td>0.676164</td>\n",
       "      <td>-0.165722</td>\n",
       "      <td>-0.037106</td>\n",
       "      <td>0.275259</td>\n",
       "      <td>0.312987</td>\n",
       "      <td>0.337004</td>\n",
       "      <td>-0.394725</td>\n",
       "      <td>0.522620</td>\n",
       "      <td>0.114512</td>\n",
       "      <td>-0.132149</td>\n",
       "      <td>0.603281</td>\n",
       "      <td>-0.432709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.818599</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>-1.211221</td>\n",
       "      <td>1.077062</td>\n",
       "      <td>1.516030</td>\n",
       "      <td>-1.075992</td>\n",
       "      <td>1.479851</td>\n",
       "      <td>-0.533473</td>\n",
       "      <td>-0.341035</td>\n",
       "      <td>-1.077543</td>\n",
       "      <td>0.093893</td>\n",
       "      <td>0.320435</td>\n",
       "      <td>0.522620</td>\n",
       "      <td>-0.743619</td>\n",
       "      <td>-0.132149</td>\n",
       "      <td>-0.355327</td>\n",
       "      <td>-0.994939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.571919</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>-0.619845</td>\n",
       "      <td>-1.490877</td>\n",
       "      <td>1.077062</td>\n",
       "      <td>-0.593108</td>\n",
       "      <td>-0.686624</td>\n",
       "      <td>-0.494837</td>\n",
       "      <td>-0.037106</td>\n",
       "      <td>-0.186962</td>\n",
       "      <td>-0.150523</td>\n",
       "      <td>0.337004</td>\n",
       "      <td>-0.037145</td>\n",
       "      <td>0.522620</td>\n",
       "      <td>-0.743619</td>\n",
       "      <td>-1.093497</td>\n",
       "      <td>-1.313935</td>\n",
       "      <td>-0.375272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Basic_Demos_Enroll_Year  Basic_Demos_Study_Site  PreInt_Demos_Fam_Child_Ethnicity  PreInt_Demos_Fam_Child_Race  MRI_Track_Scan_Location  Barratt_Barratt_P1_Edu  Barratt_Barratt_P1_Occ  Barratt_Barratt_P2_Edu  Barratt_Barratt_P2_Occ  EHQ_EHQ_Total  ColorVision_CV_Score  APQ_P_APQ_P_CP  APQ_P_APQ_P_ID  APQ_P_APQ_P_INV  APQ_P_APQ_P_OPD  APQ_P_APQ_P_PM  APQ_P_APQ_P_PP  SDQ_SDQ_Conduct_Problems  SDQ_SDQ_Difficulties_Total  SDQ_SDQ_Emotional_Problems  SDQ_SDQ_Externalizing  SDQ_SDQ_Generating_Impact  SDQ_SDQ_Hyperactivity  SDQ_SDQ_Internalizing  SDQ_SDQ_Peer_Problems  SDQ_SDQ_Prosocial  MRI_Track_Age_at_Scan\n",
       "0                        2019                       4                               1.0                          0.0                      3.0                    21.0               45.000000               17.903324               32.871543       0.818599             -0.200460       -0.619845        0.466715         0.870819        -1.195718        0.676164        0.492507                  0.459262                    0.737479                    0.776497               0.823227                   0.320435               0.880342               0.400556              -0.132149           0.603281               1.052928\n",
       "1                        2017                       1                               0.0                          9.0                      2.0                    21.0                0.000000               21.000000               45.000000       0.662301              0.277591       -0.619845       -0.372253        -0.985375         2.118640        2.233637        1.479851                  1.451997                    1.199700                    0.776497               1.309451                   0.320435               0.880342               0.686599               0.348525           0.123977               1.088973\n",
       "2                        2017                       1                               1.0                          2.0                      2.0                     9.0                0.000000               12.387745               17.897575       0.549071              0.277591       -0.619845        2.144650        -0.572888         0.009503        1.844269        0.821621                  0.459262                    1.815994                    2.167026               0.580116                   2.108335               0.522620               2.688906               2.271221          -0.355327               0.773597\n",
       "3                        2018                       1                               3.0                          8.0                      2.0                    18.0               10.000000               18.000000                0.000000       0.683936              0.277591       -0.619845       -0.651909         0.458331        -0.894413        0.676164        0.821621                 -1.029841                   -1.111402                   -1.077543              -1.121665                  -1.467465              -0.908267              -0.743619              -0.132149          -0.834631              -0.566931\n",
       "4                        2018                       1                               0.0                          1.0                      2.0                    12.0                0.000000               13.845904               22.432518      -1.203370              0.277591        3.157189       -0.372253        -0.985375         1.214724       -0.881308       -0.494837                  1.948365                    1.661920                    2.167026               1.795674                   1.393175               1.238063               0.972643              -0.612823          -1.793238              -1.572278\n",
       "...                       ...                     ...                               ...                          ...                      ...                     ...                     ...                     ...                     ...            ...                   ...             ...             ...              ...              ...             ...             ...                       ...                         ...                         ...                    ...                        ...                    ...                    ...                    ...                ...                    ...\n",
       "1208                     2019                       4                               1.0                          1.0                      3.0                    12.0               15.136452               15.000000                5.000000       0.818599              0.277591       -0.619845        0.187059        -0.779131         0.009503       -0.686624       -1.153066                  1.451997                    1.199700                    1.240007               1.309451                   0.320435               0.880342               0.686599              -0.132149           0.123977               0.018928\n",
       "1209                     2018                       1                               0.0                          0.0                      3.0                    21.0               40.000000               21.000000               40.000000      -0.259717              0.277591       -0.619845        1.305682         0.458331         0.913419       -0.297256       -0.165722                 -0.037106                   -1.111402                   -1.077543              -0.635442                  -1.467465              -0.908267              -1.315707              -1.093497          -0.355327              -1.327849\n",
       "1210                     2018                       3                               2.0                          3.0                      3.0                    21.0               40.000000               21.000000               35.000000      -2.012158              0.277591       -0.619845        1.585338        -1.191619         0.612114        0.676164       -0.165722                 -0.037106                    0.275259                    0.312987               0.337004                  -0.394725               0.522620               0.114512              -0.132149           0.603281              -0.432709\n",
       "1211                     2019                       4                               0.0                          1.0                      3.0                    18.0               35.000000               18.000000               45.000000       0.818599              0.277591       -0.619845       -1.211221         1.077062         1.516030       -1.075992        1.479851                 -0.533473                   -0.341035                   -1.077543               0.093893                   0.320435               0.522620              -0.743619              -0.132149          -0.355327              -0.994939\n",
       "1212                     2017                       1                               0.0                          0.0                      2.0                    18.0               35.000000               15.000000               35.000000       0.571919              0.277591       -0.619845       -1.490877         1.077062        -0.593108       -0.686624       -0.494837                 -0.037106                   -0.186962                   -0.150523               0.337004                  -0.037145               0.522620              -0.743619              -1.093497          -1.313935              -0.375272\n",
       "\n",
       "[1213 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_adhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c92ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Bag 1\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8363\n",
      "=> Fold 2, F1: 0.7768\n",
      "=> Fold 3, F1: 0.8480\n",
      "=> Fold 4, F1: 0.8448\n",
      "=> Fold 5, F1: 0.8343\n",
      "\n",
      "Bag 1 Avg F1: 0.8280\n",
      "#########################\n",
      "### Bag 2\n",
      "#########################\n",
      "=> Fold 1, F1: 0.7977\n",
      "=> Fold 2, F1: 0.8131\n",
      "=> Fold 3, F1: 0.8443\n",
      "=> Fold 4, F1: 0.8503\n",
      "=> Fold 5, F1: 0.8496\n",
      "\n",
      "Bag 2 Avg F1: 0.8310\n",
      "#########################\n",
      "### Bag 3\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8204\n",
      "=> Fold 2, F1: 0.8446\n",
      "=> Fold 3, F1: 0.8276\n",
      "=> Fold 4, F1: 0.8353\n",
      "=> Fold 5, F1: 0.8354\n",
      "\n",
      "Bag 3 Avg F1: 0.8326\n",
      "#########################\n",
      "### Bag 4\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8094\n",
      "=> Fold 2, F1: 0.8174\n",
      "=> Fold 3, F1: 0.8319\n",
      "=> Fold 4, F1: 0.8546\n",
      "=> Fold 5, F1: 0.8494\n",
      "\n",
      "Bag 4 Avg F1: 0.8325\n",
      "#########################\n",
      "### Bag 5\n",
      "#########################\n",
      "=> Fold 1, F1: 0.8314\n",
      "=> Fold 2, F1: 0.8408\n",
      "=> Fold 3, F1: 0.8300\n",
      "=> Fold 4, F1: 0.8319\n",
      "=> Fold 5, F1: 0.8085\n",
      "\n",
      "Bag 5 Avg F1: 0.8285\n",
      "\n",
      "##################################################\n",
      "==== Overall Results ===\n",
      "Mean F1 across bags: 0.8305 ± 0.0019\n",
      "\n",
      "Fold-wise standard deviations:\n",
      "Fold 1: 0.4646\n",
      "Fold 2: 0.4662\n",
      "Fold 3: 0.4662\n",
      "Fold 4: 0.4651\n",
      "Fold 5: 0.4651\n",
      "\n",
      "Final OOF F1: 0.8414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Initialize storage\n",
    "oof_predictions = np.zeros(len(train_adhd))\n",
    "models = {}\n",
    "bag_f1_scores = []\n",
    "stds = []\n",
    "F1s = []\n",
    "\n",
    "BAGS = 5\n",
    "FOLDS = 5\n",
    "target_col = 'ADHD_Outcome'\n",
    "\n",
    "# Prepare data\n",
    "X = train_adhd\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = train[target_col]\n",
    "\n",
    "for bag in range(BAGS):\n",
    "    print('#'*25)\n",
    "    print(f'### Bag {bag+1}')\n",
    "    print('#'*25)\n",
    "    \n",
    "    models[bag] = []\n",
    "    bag_pred = np.zeros(len(X))\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=bag*FOLDS)\n",
    "    fold_f1s = []\n",
    "    \n",
    "    for fold, (train_adhd_idx, valid_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f'=> Fold {fold}, ', end='')\n",
    "\n",
    "        # Model train_adhding WITHOUT downsampling\n",
    "        model_adhd = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.175,\n",
    "            num_leaves=31,\n",
    "            reg_alpha=0.0,\n",
    "            reg_lambda=0.0009,\n",
    "            feature_fraction=0.1,\n",
    "            subsample=0.8,\n",
    "            verbose=-1,\n",
    "            class_weight='balanced'  # Handling imbalance through class weights\n",
    "        )\n",
    "        model_adhd.fit(X.iloc[train_adhd_idx], y.iloc[train_adhd_idx])\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = model_adhd.predict(X.iloc[valid_idx])\n",
    "        fold_f1 = f1_score(y.iloc[valid_idx], pred)\n",
    "        fold_f1s.append(fold_f1)\n",
    "        print(f\"F1: {fold_f1:.4f}\")\n",
    "        \n",
    "        # Store predictions\n",
    "        bag_pred[valid_idx] = model_adhd.predict_proba(X.iloc[valid_idx])[:, 1]\n",
    "        models[bag].append(model_adhd)\n",
    "        \n",
    "        # Store validation set std for this fold\n",
    "        stds.append(y.iloc[valid_idx].std())\n",
    "    \n",
    "    # Bag-level results\n",
    "    bag_avg_f1 = np.mean(fold_f1s)\n",
    "    bag_f1_scores.append(bag_avg_f1)\n",
    "    print(f\"\\nBag {bag+1} Avg F1: {bag_avg_f1:.4f}\")\n",
    "    oof_predictions += bag_pred / BAGS\n",
    "    F1s.append(fold_f1s)\n",
    "\n",
    "# Final evaluation\n",
    "print('\\n' + '#'*50)\n",
    "print(\"==== Overall Results ===\")\n",
    "print(f\"Mean F1 across bags: {np.mean(bag_f1_scores):.4f} ± {np.std(bag_f1_scores):.4f}\")\n",
    "\n",
    "print(\"\\nFold-wise standard deviations:\")\n",
    "for i, std in enumerate(stds[:FOLDS]):  # Show first fold's std as representative\n",
    "    print(f\"Fold {i+1}: {std:.4f}\")\n",
    "\n",
    "# Final OOF score\n",
    "final_preds = (oof_predictions > 0.5).astype(int)\n",
    "final_f1 = f1_score(y, final_preds)\n",
    "print(f\"\\nFinal OOF F1: {final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10444949",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65676e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only the columns from `trf` that are present in `train`\n",
    "columns_to_drop = [col for col in trf.columns if col in train.columns]\n",
    "test_adhd = test.drop(columns=columns_to_drop)\n",
    "\n",
    "# Drop the target columns\n",
    "test_adhd = train_adhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adhd['paticipant_id'] = test['participant_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553a54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  participant_id  adhd_probability  adhd_prediction  prediction_std  \\\n",
      "0   Cfwaf5FX7jWK          0.986018                1        0.004542   \n",
      "1   vhGrzmvA3Hjq          0.966732                1        0.009391   \n",
      "2   ULliyEXjy4OV          0.294029                0        0.011514   \n",
      "3   LZfeAb1xMtql          0.156272                0        0.030929   \n",
      "4   EnFOUv0YK1RG          0.991727                1        0.001153   \n",
      "\n",
      "   confidence_score  bag_agreement  \n",
      "0          0.995458            1.0  \n",
      "1          0.990609            1.0  \n",
      "2          0.988486            0.0  \n",
      "3          0.969071            0.0  \n",
      "4          0.998847            1.0  \n"
     ]
    }
   ],
   "source": [
    "def predict_adhd(test_adhd, models, feature_columns=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained bagged LGBM model for ADHD\n",
    "    \n",
    "    Args:\n",
    "        test_adhd: DataFrame containing the same features as training data\n",
    "        models: Dictionary of trained models from your training code\n",
    "        feature_columns: List of feature names used in training (optional)\n",
    "        threshold: Classification cutoff (default=0.5)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions and confidence metrics\n",
    "    \"\"\"\n",
    "    # Preprocess new data\n",
    "    X_new = pd.get_dummies(test_adhd, drop_first=True)\n",
    "    \n",
    "    # If feature columns not provided, infer from first model\n",
    "    if feature_columns is None:\n",
    "        feature_columns = models[0][0].feature_name_\n",
    "    \n",
    "    # Ensure feature alignment\n",
    "    missing_features = set(feature_columns) - set(X_new.columns)\n",
    "    extra_features = set(X_new.columns) - set(feature_columns)\n",
    "    \n",
    "    for f in missing_features:\n",
    "        X_new[f] = 0  # Add missing features with 0\n",
    "    X_new = X_new[feature_columns]  # Reorder and remove extras\n",
    "    \n",
    "    # Initialize storage\n",
    "    bag_predictions = np.zeros((len(X_new), BAGS))\n",
    "    oof_predictions = np.zeros(len(X_new))\n",
    "    \n",
    "    # Make predictions from each bag\n",
    "    for bag in range(BAGS):\n",
    "        # Average predictions across folds within bag\n",
    "        bag_pred = np.zeros(len(X_new))\n",
    "        for fold_model in models[bag]:\n",
    "            bag_pred += fold_model.predict_proba(X_new)[:, 1] / FOLDS\n",
    "        bag_predictions[:, bag] = bag_pred\n",
    "        oof_predictions += bag_pred / BAGS\n",
    "    \n",
    "    # Calculate uncertainty metrics\n",
    "    pred_std = np.std(bag_predictions, axis=1)\n",
    "    confidence = 1 - pred_std  # Higher value = more confident\n",
    "    \n",
    "    # Ensure 'participant_id' exists in test_adhd\n",
    "    if 'participant_id' not in test_adhd.columns:\n",
    "        raise KeyError(\"'participant_id' column is missing in the input data.\")\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'participant_id': test_adhd['participant_id'],\n",
    "        'adhd_probability': oof_predictions,\n",
    "        'adhd_prediction': (oof_predictions > threshold).astype(int),\n",
    "        'prediction_std': pred_std,\n",
    "        'confidence_score': confidence,\n",
    "        'bag_agreement': (bag_predictions > threshold).mean(axis=1)\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage -------------------------------------------------\n",
    "\n",
    "# 1. Get feature columns (run after training)\n",
    "feature_columns = models[0][0].feature_name_  # From first model\n",
    "\n",
    "# 2. Ensure 'participant_id' column exists in test_adhd\n",
    "if 'participant_id' not in test_adhd.columns:\n",
    "    test_adhd['participant_id'] = test['participant_id']  # Add 'participant_id' from the original test DataFrame\n",
    "\n",
    "# 3. Make predictions\n",
    "predictions = predict_adhd(test_adhd, models, feature_columns)\n",
    "\n",
    "# 4. Save/use results\n",
    "print(predictions.head())\n",
    "# predictions.to_csv('adhd_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only the columns from `trf` that are present in `train`\n",
    "columns_to_drop = [col for col in trf.columns if col in test.columns]\n",
    "test_adhd = test.drop(columns=columns_to_drop)\n",
    "\n",
    "# Drop the target columns\n",
    "test_adhd = test_adhd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92562b51",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Ensure all arrays have the same length\n",
    "if len(test_ids) == len(test_preds_sex_labels) == len(adhd_predictions_binary):\n",
    "    # Create a DataFrame for submission\n",
    "    submission = pd.DataFrame({\n",
    "        'participant_id': test_ids,\n",
    "        'ADHD_Outcome': adhd_predictions_binary,\n",
    "        'Sex_F': test_preds_sex_labels,\n",
    "\n",
    "    })\n",
    "\n",
    "    # Save the submission file\n",
    "    submission.to_csv('submission_adhd_84_sex_68.csv', index=False)\n",
    "    print(\"Submission file created successfully!\")\n",
    "else:\n",
    "    print(\"Error: Length mismatch between test_ids, test_preds_sex_labels, and adhd_prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506cdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ADHD F1 (harmonic mean): 0.8476\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import hmean\n",
    "\n",
    "# Using harmonic mean of bag scores (though mathematically similar for single metric)\n",
    "final_f1_hmean = hmean(bag_f1_scores)  \n",
    "print(f\"Final ADHD F1 (harmonic mean): {final_f1_hmean:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
